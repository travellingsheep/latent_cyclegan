experiment:
  name: "new_baseline"
  # outputs_dir: "outputs/stargan_v2_baseline_id2"

model:
  latent_channels: 4
  style_dim: 64
  base_dim: 256
  num_domains: 5
  n_res_blocks: 6
  d_layers: 3

  latent_dim: 16
  mapping_hidden_dim: 512
  mapping_shared_layers: 3

loss:
  w_adv: 1.0
  w_sty: 1.0
  w_ds: 1.0
  ds_margin: 1.5   # diversity loss 截断上限；null/不填表示不截断
  w_cyc: 1.0
  w_id: 1.0
  w_r1: 2.0
  r1_interval: 16
  ds_decay_epochs: 20

training:
  seed: 42
  batch_size: 64
  num_epochs: 100
  lr_g: 1.0e-4
  lr_d: 1.0e-4
  lr_mapping: 1.0e-6
  weight_decay: 1.0e-4
  lr_schedule: none     # none | cosine | linear (step-based)
  warmup_steps: 200
  min_lr_ratio: 0.1
  use_amp: true
  amp_dtype: bf16
  allow_tf32: true
  cudnn_benchmark: true
  optim_fused: true
  latent_scale: 0.18215
  num_workers: 0
  pin_memory: true
  use_compile: true
  compile_mode: default
  compile_dynamic: false
  compile_fullgraph: false
  compile_cudagraphs: false
  compile_generator: true
  compile_mapping_network: true
  compile_style_encoder: true
  compile_discriminator: false
  drop_last: true
  progress_bar: false
  progress_style: percent
  display_interval: 500
  log_json: true
  log_interval: 50
  save_interval: 1
  # eval_interval: 10
  console_log_path: "{outputs_dir}/console.log"

visualization:
  every_epochs: 1
  vae_model_name_or_path: "runwayml/stable-diffusion-v1-5"
  vae_subfolder: "vae"
  suppress_hf_warnings: true
  num_samples: 5
  save_dir: "{outputs_dir}/vis"
  # random_noise 模式：固定种子采样一次 z，并在整个训练/离线评估过程中复用。
  random_noise_seed: 42

  # 离线评估输出目录（每次实验目录下的 eval/）
  eval_dir: "{outputs_dir}/eval"
  # 训练结束后，自动使用 latest.pt 做一次离线可视化（ref + random_noise）。
  auto_eval_latest: true

data:
  data_root: "../pt_datasets/unscaled_dataset"
  # 离线 eval 使用的测试集（同样是按 domain 子文件夹组织的 .pt/.npy latents）
  # 如果留空/不填，则离线 eval 会退回使用训练集 data_root。
  eval_data_root: "../pt_datasets/unscaled_test_dataset"
  domains:
    - photo
    - Hayao
    - monet
    - cezanne
    - vangogh
  preload_to_gpu: true
  # 控制每个 epoch 的样本数（从而控制 steps/epoch），用于更快迭代。
  # 你机器上当前约 1.26s/step，想 <2min/epoch 且 batch=64 时，可先用 6144（约 96 steps）。
  epoch_size: 6144

checkpoint:
  save_dir: "{outputs_dir}/ckpt"
  resume_path: "{outputs_dir}/ckpt/latest.pt"
